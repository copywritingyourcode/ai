Thanks for the details! I’ll draft a comprehensive PRD for your AI local LLM setup, ensuring it is understandable for a junior Python developer. The PRD will include:

- AI interaction through a CLI.
- Support for multiple models from your available Ollama list.
- The ability to read and process PDF and TXT files with effective chunking and indexing.
- A memory layer using ChromaDB for permanent conversation storage.
- A robust testing script to debug AI responses and general software issues.
- A concentrator and splitter script to structure project code into text for AI analysis.

I will structure the PRD in a way that ensures clarity and usability. I’ll get back to you once it’s ready.

# Local AI Assistant with Persistent Memory and Debugging

## Overview

This document describes a **local AI-powered assistant** that runs entirely on a high-performance Apple M4 Max machine. The assistant uses multiple local large language models (LLMs) via the **Ollama** platform and interacts with the user through a command-line interface (CLI). Key capabilities include **persistent conversation memory** (using a vector database to remember all past chats), **retrieval-augmented generation (RAG)** from user-provided documents (PDFs/TXT files), and tools for **automated debugging** of AI outputs and code. The goal is to create a system that can engage in continuous, context-rich dialogue, incorporate knowledge from documents in real-time, and assist in diagnosing and structuring code, all while running locally for privacy and efficiency.

## Core Features

### AI Chat Interface (CLI-Based)

- **Interactive Chat:** The assistant provides a CLI where the user can type messages and receive responses from a local LLM. The primary model is **Gemma3:27B**, a 27-billion-parameter LLM from Google's Gemini family, known for its strong reasoning and long context window ([gemma3:27b](https://ollama.com/library/gemma3:27b#:~:text=The%20Gemma%203%20models%20are,compact%20design%20allows%20deployment%20on)). The interface should support switching between multiple models as needed (e.g., for different tasks or resource constraints).

- **Conversation Memory:** All user queries and AI answers are remembered beyond the immediate context. Instead of losing earlier conversation when the context window fills up, the system will log and embed each interaction into a **ChromaDB** vector database. This persistent memory allows the AI to **recall events or facts from any past conversation**, essentially giving it a long-term memory. When generating a response, the assistant can retrieve relevant past messages from the database and include them in the context, ensuring continuity even across long sessions.

- **Configurable Context Window:** The chat interface will allow configuration of how much context to include in each prompt. Since Gemma3 supports up to 128K tokens of context ([gemma3:27b](https://ollama.com/library/gemma3:27b#:~:text=The%20Gemma%203%20models%20are,compact%20design%20allows%20deployment%20on)), the system can potentially include very long histories or document excerpts. However, for efficiency, the developer can set a limit (for example, only the last N messages plus any highly relevant older ones). This helps optimize response quality and speed by not overloading the model with unnecessary text. The context window setting can be adjusted based on the model in use (smaller models may have shorter context limits).

- **Multi-Model Support:** While Gemma3:27B is the default for chat, the CLI should let the user choose other local models available via Ollama. For instance, a smaller model could be used for quick replies, or a specialized model for coding tasks. The implementation might include a command or flag (e.g., `/use <model_name>`) to load a different model, ensuring flexibility. All models run through Ollama’s local inference engine, which is optimized for Mac hardware.

### Document Handling

- **PDF and Text Ingestion:** The assistant can accept PDF and text files uploaded by the user for reference. It will read these documents and convert them into text data. For PDFs, the **PyMuPDF** library (via LangChain’s PyMuPDFLoader) will be used for fast and accurate text extraction ([GPT-4 and LangChain: Building Python Chatbot with PDF Integration | Next Idea Tech Blog](https://blog.nextideatech.com/chat-with-documents-using-langchain-gpt-4-python/#:~:text=With%20PyMuPDFLoader%2C%20you%20can%20load,returns%20one%20document%20per%20page)). PyMuPDF is chosen because it’s one of the fastest PDF parsing options and preserves detailed metadata (e.g. page numbers) while extracting text ([GPT-4 and LangChain: Building Python Chatbot with PDF Integration | Next Idea Tech Blog](https://blog.nextideatech.com/chat-with-documents-using-langchain-gpt-4-python/#:~:text=With%20PyMuPDFLoader%2C%20you%20can%20load,returns%20one%20document%20per%20page)). For plain `.txt` files, a simple file read is sufficient.

- **Chunking and Indexing:** Large documents are broken into smaller chunks to facilitate efficient search and retrieval. The system will implement a chunking strategy that keeps related content together. According to LangChain documentation, splitting text into **semantically meaningful chunks** (e.g. by paragraphs or sections) with a bit of overlap yields the best results ([GPT-4 and LangChain: Building Python Chatbot with PDF Integration | Next Idea Tech Blog](https://blog.nextideatech.com/chat-with-documents-using-langchain-gpt-4-python/#:~:text=The%20concept%20of%20TextSplitters%20revolves,different%20approaches%20to%20achieve%20this)) ([GPT-4 and LangChain: Building Python Chatbot with PDF Integration | Next Idea Tech Blog](https://blog.nextideatech.com/chat-with-documents-using-langchain-gpt-4-python/#:~:text=chunk_overlap%20parameter%20refers%20to%20the,avoids%20abrupt%20transitions%20between%20chunks)). For example, we might split documents into chunks of about 500 tokens with some overlapping sentences between chunks to maintain context continuity. This can be achieved using LangChain’s text splitter utilities (such as `RecursiveCharacterTextSplitter` ([GPT-4 and LangChain: Building Python Chatbot with PDF Integration | Next Idea Tech Blog](https://blog.nextideatech.com/chat-with-documents-using-langchain-gpt-4-python/#:~:text=from%20langchain))), which combine sentences until a chunk size limit is reached and ensure chunks don’t cut off in the middle of a sentence or word.

- **Vector Embeddings and Storage:** Each text chunk from the documents will be converted into a vector embedding for semantic search. We will use the **Nomic-embed-text** model to generate embeddings. This is a high-performance open-source text encoder that outperforms OpenAI’s embedding models on many tasks ([nomic-embed-text](https://ollama.com/library/nomic-embed-text#:~:text=%60nomic,short%20and%20long%20context%20tasks)). Using Ollama’s API, the system can generate an embedding for each chunk and then store those vectors in ChromaDB along with metadata (like document name and chunk location). Storing embeddings with metadata allows filtering results (e.g., you could retrieve only from a specific document if needed).

- **Real-Time Document Querying:** Once documents are indexed, the assistant can answer user questions using the documents’ information. When the user asks something that might be in the uploaded files, the system performs a similarity search in the ChromaDB index to find the most relevant chunks. The top relevant pieces of text are then retrieved and provided to the LLM as part of the prompt (this is the Retrieval-Augmented Generation approach). The assistant’s answer will be based not only on its trained knowledge but also on the actual content of the user’s documents. This ensures up-to-date and specific answers. For instance, if the user asks “What does the design document say about chunk size?”, the system will find the chunk in the documents where chunking is described and feed that into the model to generate an accurate answer. We can use the **DeepSeek-RAG** model for this answering step if needed – DeepSeek is a chain-of-thought LLM tuned for retrieval tasks ([Testing DeepSeek R1 locally for RAG with Ollama and Kibana - Elasticsearch Labs](https://www.elastic.co/search-labs/blog/deepseek-rag-ollama-playground#:~:text=Everyone%20is%20talking%20about%20DeepSeek,of%20Deepseek%20R1%20for%20RAG)), which may improve how the answer cites or uses document facts. If DeepSeek is used, the system would formulate a prompt that includes the retrieved text and let DeepSeek reason about it to answer the question.

### Memory Layer (ChromaDB)

- **Persistent Conversational Memory:** The assistant’s memory system is built on **ChromaDB**, an open-source vector database for embeddings. ChromaDB allows us to store and retrieve text embeddings efficiently and persistently ([Learn How to Use Chroma DB: A Step-by-Step Guide | DataCamp](https://www.datacamp.com/tutorial/chromadb-tutorial-step-by-step-guide#:~:text=Chroma%20DB%20is%20an%20open,search%20engines%20over%20text%20data)). Every user message and AI response will be transformed into an embedding (using the same Nomic model) and stored in a ChromaDB collection dedicated to conversation history. By storing the conversation as vectors, we enable semantic recall: the assistant can later query “Have we talked about X before?” by searching this vector store for related vectors instead of having to literally store and scan full text.

- **Combining Memory and Documents:** The memory database can be integrated with the document embeddings to enrich the assistant’s responses. In practice, we might maintain separate collections (one for dialogue memory, one for documents) but query both when appropriate. For example, if the user asks a question that references something from an earlier conversation, the system will search the conversation embeddings for similar context. If the question is about an uploaded document, the system will search the document embeddings. In some cases, both may be relevant (a question that builds on past discussion and needs document info), so the assistant can retrieve from both sources. **ChromaDB** supports metadata and multiple collections, so we can tag each entry (conversation vs document) and fetch accordingly. The ability to retrieve by semantic similarity means the assistant’s "long-term memory" isn't just a verbatim log but a **meaning-aware** memory – it can recall the gist of previous talks even if phrased differently ([Chroma Tutorial: How to give GPT-3.5 chatbot memory-like capability tutorial](https://lablab.ai/t/chroma-tutorial-with-openais-gpt-35-model-for-memory-feature-in-chatbot#:~:text=In%20this%20particular%20case%2C%20embeddings,the%20database%20using%20vector%20representations)).

- **Memory Retrieval in Practice:** When forming a new prompt for the LLM, the system will merge recent conversation context with a few top relevant past entries fetched from ChromaDB. For instance, it might always include the last few exchanges verbatim (to maintain immediate context), plus any older exchanges that are similar to the user’s current request (to remind the model of prior knowledge or decisions). By storing the history forever and retrieving on demand, we avoid hitting context length limits while still keeping important information accessible. ChromaDB’s persistent storage (on disk) ensures that even if the program restarts, the conversation history is not lost and can be reloaded.

### Testing & Debugging Framework

- **Automated Response Evaluation:** A dedicated testing script will be developed to catch errors or issues in the AI’s outputs. This framework will run alongside the chat system (or be invoked on demand) to analyze the content of the assistant’s responses. For example, if the assistant produces a piece of Python code as an answer, the testing script can attempt to execute that code in a sandbox and detect runtime errors or exceptions. If the assistant gives a factual answer, the script might compare it against the documents or a known dataset (if available) to flag possible inaccuracies. The goal is to identify bugs or incorrect information automatically, rather than relying solely on the user or developer to spot them.

- **Issue Identification and Logging:** The debugging script will log detailed diagnostic information whenever an issue is found. It will categorize the type of issue for clarity. Some anticipated categories include:
  - *Syntax or Runtime Errors:* If generated code fails to run (e.g., Python syntax error or exception), the log will capture the error message and the portion of code that caused it.
  - *Incorrect Factual Answers:* If the assistant’s answer contradicts the reference documents or known facts, the tool notes it (potentially by cross-checking the answer against the vector store or an external knowledge base).
  - *Formatting Issues:* If the response is not following a desired format (for instance, missing markdown formatting for code blocks or lists), the script can flag this as a minor issue.
  - *Performance Problems:* If the assistant took too long or used an excessive number of tokens for a simple query, that might be logged as an efficiency issue.

- **Debugging Recommendations:** Beyond just logging issues, the framework will provide **automatic recommendations** for fixes when possible. This could be implemented by leveraging an LLM (like a smaller model or even Gemma3 itself) in analysis mode: e.g., feed the error log or problematic response to a prompt like *“What went wrong and how to fix it?”*. Because Gemma3 is good at reasoning ([gemma3:27b](https://ollama.com/library/gemma3:27b#:~:text=The%20Gemma%203%20models%20are,compact%20design%20allows%20deployment%20on)), it could offer suggestions such as “The code failed because of a null pointer; consider checking if the variable is None before use.” For factual errors, it might suggest “The assistant should double-check this answer against the documents, as the data might be outdated or misremembered.” These recommendations would be included in the diagnostic report to guide developers in refining prompts or improving the system.

- **General Software Debugging:** The same script can be used to debug the assistant’s own codebase or other software. For instance, it could read log files of the assistant system or run static analysis on the code. It would then use patterns or LLM analysis to pinpoint potential issues (memory leaks, unhandled exceptions, etc.). The output would be a structured report (possibly in a log file or printed to console) that a developer can review to quickly understand any problems in both the AI’s responses and the underlying system.

### Code Processing (Concentrator & Splitter Script)

- **Code Concentrator:** To facilitate external review or analysis of the entire project’s code, a script will gather all source code into one structured text output. This “concentrator” will traverse the project directory (skipping irrelevant files like dependencies or large binary assets) and concatenate code files into a single document. It will insert clear delimiters between files (for example, a header like `\n\n### File: src/module_a.py ###\n` before that file’s content) so that the structure of the project is preserved in the text. This combined code document allows an LLM to ingest the whole codebase sequentially if it has a large enough context window (Gemma3 could handle it if under 128K tokens, otherwise another model or chunking will be used).

- **Code Splitting:** If the total codebase is too large for one prompt, the splitter component will break the consolidated code into manageable chunks. The splitting will be mindful of **functionality boundaries and token limits**. For example, it will attempt to split by module or by classes if possible, rather than cutting a class or function in half. We can utilize a token counter (like tiktoken for GPT-style BPE tokens) or simply character count as an approximation, to ensure each chunk is within a safe size (perhaps 8,000 tokens per chunk for general compatibility, or tailored to the target analysis model’s context size). There may be slight overlaps or repeated header lines between chunks to provide context (e.g., repeating the file header if a file’s content is split into two parts) similar to how text splitters include overlap ([GPT-4 and LangChain: Building Python Chatbot with PDF Integration | Next Idea Tech Blog](https://blog.nextideatech.com/chat-with-documents-using-langchain-gpt-4-python/#:~:text=chunk_overlap%20parameter%20refers%20to%20the,avoids%20abrupt%20transitions%20between%20chunks)). This ensures that an LLM analyzing one chunk has some context of what came just before.

- **Structured for AI Analysis:** The output of the splitter will be a series of files or strings that can be fed into an LLM one by one, or as part of an automated analysis pipeline. For instance, if using an external service or a more powerful model for code review, the script might output a numbered sequence of chunks (like `code_chunk_01.txt`, `code_chunk_02.txt`, etc.). The structure (with file names and sections clearly indicated) helps the AI to know where it is in the project. This is **optimized for external analysis**, meaning a developer could take these chunks and ask an LLM like GPT-4 or Code LLMs to "review the code for bugs or improvements" chunk by chunk. Because each piece is labeled and structured, the LLM could even refer to "File X, function Y" in its feedback. Essentially, the concentrator & splitter ensure that the entire project’s code can be ingested by an AI in pieces, while preserving readability and logical segmentation.

- **Use Cases:** This feature is useful for conducting thorough code reviews, generating documentation, or searching for vulnerabilities using AI. A junior developer could run the concentrator to produce a single text of the code, use the splitter to break it down, and then feed those to the AI to get insights about the codebase. It saves time by automating the preparation of code for analysis.

## Technical Requirements

- **Platform:** The system will run on a **local Apple M4 Max** machine with 128 GB RAM and 2 TB SSD. This hardware is quite powerful and can accommodate large models like a 27B parameter LLM. The Mac’s Neural Engine and GPU can be utilized via Ollama to accelerate model inference. Sufficient disk space is available to store several models (which can be tens of GB each) and the persistent database files. No internet connection is required for the core functionality, as all models and data are stored and executed locally.

- **Programming Language & Frameworks:** The implementation will be in **Python 3.10+**. Key libraries include:
  - **Ollama API** (or CLI calls) for loading and running local models.
  - **LangChain** for high-level orchestration, especially for document loading, text splitting, and possibly managing the conversational chain.
  - **ChromaDB** for the vector memory (either using the `chromadb` Python package or via LangChain’s integration). Chroma is an AI-native, open-source vector store that saves embeddings with metadata and allows efficient similarity search ([Learn How to Use Chroma DB: A Step-by-Step Guide | DataCamp](https://www.datacamp.com/tutorial/chromadb-tutorial-step-by-step-guide#:~:text=Chroma%20DB%20is%20an%20open,search%20engines%20over%20text%20data)).
  - **PyMuPDF (fitz)** for PDF text extraction, likely used through LangChain’s `PyMuPDFLoader` for convenience.
  - Additional libraries: possibly **tiktoken** (for token counting), **logging** (for debug logs), and standard Python modules for file I/O and subprocess management.

- **Ollama Models Used:** The project will leverage multiple models obtainable through Ollama:
  - **Gemma3:27B** – Primary assistant LLM for general conversation. It’s a 27B parameter model from Google’s Gemma family (based on Gemini) with multimodal capabilities and a 128k token context window ([gemma3:27b](https://ollama.com/library/gemma3:27b#:~:text=The%20Gemma%203%20models%20are,compact%20design%20allows%20deployment%20on)). This model excels at chat, reasoning, summarization, and Q&A tasks, making it ideal for the core chat functionality.
  - **DeepSeek-RAG** – A specialized model for retrieval-augmented generation. DeepSeek is a chain-of-thought reasoning model with open weights, introduced for its strong performance in RAG scenarios ([Testing DeepSeek R1 locally for RAG with Ollama and Kibana - Elasticsearch Labs](https://www.elastic.co/search-labs/blog/deepseek-rag-ollama-playground#:~:text=Everyone%20is%20talking%20about%20DeepSeek,of%20Deepseek%20R1%20for%20RAG)). We will use it when the task is heavily focused on integrating retrieved documents or when complex reasoning with provided facts is needed. For example, the assistant might hand off a user’s question plus relevant docs to DeepSeek to produce a well-reasoned answer that cites those docs.
  - **Nomic-embed-text** – An embedding model used to vectorize text for memory and document search. This model provides high-quality text embeddings, reportedly surpassing the performance of OpenAI’s popular embedding model Ada in both short and long text scenarios ([nomic-embed-text](https://ollama.com/library/nomic-embed-text#:~:text=%60nomic,short%20and%20long%20context%20tasks)). It will be called to generate embeddings for conversation messages and document chunks before storing them in ChromaDB.
  - **Additional Models:** The design allows plugging in other models as needed. For instance, one might integrate a code-focused LLM for the debugging feature (to analyze code outputs) or smaller models for lightweight tasks. The system’s modular interface with Ollama means any model in the Ollama library can be downloaded and used with minimal changes (given enough hardware resources). This future-proofs the assistant to use improved models or domain-specific models as they become available.

- **Persistence and Data Storage:** Besides model files, the system will persist data locally. ChromaDB will store its collections (conversation memory and documents) likely in a directory on disk (using Chroma’s `persist_directory` option). Log files for debugging and any structured outputs (like the code chunks from the splitter) will also reside on the local disk. Proper file paths and management will be implemented to organize these (e.g., a folder for logs, a folder for vector store data). All data remains local to maintain privacy.

- **Security Considerations:** Running local avoids external data leaks, but we will still implement any necessary security for file handling (e.g., not executing arbitrary code from PDFs). The debugging script running code will be sandboxed or limited to safe operations to prevent any harmful actions from executed AI-generated code. Since models are running locally, we must ensure the system doesn’t overload the hardware (e.g., avoid running too many models simultaneously exceeding RAM). The platform has ample RAM, but we will test memory usage especially with the 27B model and large context inputs.

## Expected Behavior

- **Seamless Conversation Flow:** The user should experience a smooth chat interaction. They can ask a question or give a command in the CLI, and the assistant responds in a coherent and context-aware manner. If the user refers to something mentioned an hour or days ago, the assistant recognizes it thanks to the stored memory. For example, if a user says, “Can you summarize the plan we discussed last week?”, the assistant should retrieve the summary of that earlier conversation from ChromaDB and produce an answer containing those details.

- **Document-Aware Responses:** When the user queries information that is contained in an uploaded document, the assistant’s answer should reflect the document’s content. It might quote or paraphrase the relevant document chunk and even cite the document if appropriate. The response should make it clear that it is using the document knowledge (for instance, “According to the design.pdf you provided, the system uses a 512-token chunk size for splitting.”). If information is not in the documents, the assistant should either say it doesn’t know or use its general LLM knowledge, but the integration should prevent it from hallucinating facts that contradict the provided documents.

- **Persistent Knowledge Integration:** The assistant should intelligently combine memory and new data. If a user has a multi-turn conversation involving both personal queries and document queries, the assistant should handle context switches gracefully. For example, after discussing a PDF’s content, if the user asks a personal question like “Also, what’s your name?”, the assistant remembers its configured persona or previous introduction. Later, if the user goes back to “Now, in that PDF, what was step 3?”, the assistant recalls which PDF was discussed and fetches step 3. All of this should happen without the user having to manually restate context.

- **Accurate Debugging Feedback:** When the debugging framework is used, it should output clear and useful information. For instance, if the assistant’s last answer included a Python code snippet and that snippet has a bug, the system might follow up (perhaps even automatically in the CLI) with a message like: *“[Debug] Detected an error in the provided code: NameError at line 5. It seems a variable `x` is not defined. You might need to define `x` before using it.”* This feedback allows the user (or the developer of the AI) to correct mistakes. The debugging output might be prefixed or displayed in a different format to distinguish it from the assistant’s main replies, so it’s clear this is a diagnostic message.

- **Well-Structured Code Output:** When the code concentrator and splitter are run, the resulting output should be organized and easy to verify. A developer looking at the combined code text should see all files and their contents properly labeled. If the codebase was split into, say, 3 chunks due to size, each chunk should clearly note which part of the code it contains. An external code analysis on these chunks should be able to proceed chunk by chunk without confusion. Essentially, the expected behavior is that **no parts of the code are lost or jumbled** in the process — the structure remains intact. If a junior developer later uses these chunks to get an AI code review, the feedback they get will reference the correct file names and lines, thanks to the preserved structure.

- **Robust Performance:** Given the powerful hardware, the assistant should handle reasonably large tasks without crashing. For example, it should manage a conversation that goes on for hundreds of turns (with memory retrieval ensuring the model sees only what it needs at each turn). It should also handle a situation where the user uploads several large PDFs for reference — indexing might take some time, but it should complete and subsequent queries should be answered using those documents. The system should avoid running out of memory by loading models one at a time as needed (unloading others if necessary), and by controlling the size of context passed to models. If something does go wrong (e.g., model runs out of memory), it should fail gracefully (perhaps an error message to the user, and instructions to reduce load or switch models).

## Development & Testing

- **Modular Implementation:** The development will be broken into clear steps to ensure each component works before integrating the next. For instance, start by implementing the CLI chat loop with a simple local model. Once basic Q&A works, add the ChromaDB memory integration. Next, implement document ingestion and RAG, and so on. Each feature (memory, RAG, debugging, code processing) will be developed as a separate module or class, which makes it easier to test in isolation. This step-by-step approach ensures that if an issue arises, it’s easier to pinpoint which module is responsible.

- **Continuous Testing with Logs:** Throughout development, extensive logging will be enabled to trace the assistant’s internal decisions. For example, when a query is made, the system might log messages like “Searching memory for relevant context...” or “Embedding 3 document chunks for query...”. These logs serve as a built-in diagnostics tool to understand how the system arrived at its answer. The testing & debugging framework will be used during development to catch problems early. After each major feature, some test cases will be run:
  - **Unit Tests:** e.g., test the PDF loader on a sample PDF to ensure text extraction and chunking works as expected.
  - **Integration Tests:** e.g., simulate a conversation where the user asks something from a document to see if the correct retrieval is happening.
  - The detailed logs and any diagnostic reports from the debugging script will be reviewed to verify that each component not only works but is doing so efficiently (no unnecessary overhead or errors hidden under the hood).

- **Performance Optimization:** With a high-end local machine, we have a lot of headroom, but we will still optimize for smooth execution. This includes:
  - Loading models in the most efficient way (using Ollama which manages models in memory and on disk, possibly using quantized versions to save memory).
  - Ensuring that vector searches in ChromaDB are fast (for example, using appropriate indexes or limiting the search scope when possible).
  - Avoiding memory leaks by reusing objects or clearing old data not needed. Since the conversation memory grows indefinitely, we rely on ChromaDB to handle large collections, and we will periodically checkpoint or compact the database if needed.
  - Using asynchronous calls or background threads for operations like embedding generation and database insertion, so that these don’t block the main conversation flow. For instance, the assistant might respond with “Let me analyze the document...” and perform the chunking and embedding in the background, then indicate when it’s ready for queries.
  - Testing with large loads, such as a very long conversation or a very large document, to ensure the system remains responsive. If any bottlenecks are found (CPU usage, disk I/O, etc.), adjustments will be made (like chunk size tuning, or batching embeddings).

- **User Acceptance Testing:** Given this is intended for a junior developer to use or extend, some end-to-end tests will simulate how a user would interact. This includes starting the CLI, asking varied questions, uploading a file, and invoking the debugging tool. The outcomes will be checked against the expected behavior described above. The interface and outputs will be refined based on these tests to ensure clarity (for example, making sure that the debug messages are clearly distinguished and the overall chat remains user-friendly).

- **Documentation and Clarity:** As a final step, clear documentation (and in-code comments) will be written, given the target audience of a junior developer. This document itself serves as a high-level guide, and the implementation will include a README explaining how to run the assistant, how to add new models, and how each component works. Throughout development, we aim to keep the code modular and readable, so that a new developer can understand the system component by component with the help of this document and inline explanations.

By following these development and testing practices, we will create a robust local AI assistant that meets the requirements and is maintainable for future improvements. Each feature will be verified for correctness and performance, resulting in a reliable tool that a developer can confidently use and build upon.